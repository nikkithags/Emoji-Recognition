{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Emoji Recognition With web cam**"},{"metadata":{},"cell_type":"markdown","source":"*The necessary imports*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport imutils\nimport numpy as np\nfrom PIL import Image\nfrom tensorflow.keras.models import load_model\nimport imageio as ig\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Loading the model, labels and emoji pictures*"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('emojirecog.hdf5')    #pre-trained model\nbg = None\ncam = cv2.VideoCapture(0)\ntop, right, bottom, left = 80, 400, 275, 640     #specifying emoji box location\nra_weight = 1\nlabels = ['call_me','fingers_crossed','index_up','okay','paper','rock','rock_on','scissor','spock','thumbs_up']   # emoji labels for identification\nemojis = []\nfor i in labels:\n    ran = cv2.imread(\"/emojis/\" + i + \".png\",cv2.COLOR_BGR2RGB)    #to read each of the 10 emoji pictures using the labels \n    emojis.append(np.array(ran))\n    \nemojis = np.array(emojis)      # the emoji's pictures ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Function to form back ground image for comparison from the first 50 frames*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_avg(image, ra_weight):\n    global bg\n    if bg is None:\n        bg = image.copy().astype(\"float\")\n        return\n    cv2.accumulateWeighted(image, bg, ra_weight)  #calculates the running average for the frames","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Function to form the subtracted image from hand and background*"},{"metadata":{"trusted":false},"cell_type":"code","source":"def segment(image):\n    global bg\n    diff = cv2.absdiff(image,bg.astype(\"uint8\"))    #difference between background and current frame\n    threshold = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)[1]        #keep pixels only within threshold\n    cnts = cv2.findContours(threshold.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]    #finding the contours from the modified frame\n    if len(cnts) == 0:\n        return\n    else:\n        segmented = max(cnts, key=cv2.contourArea)\n        return (threshold, segmented) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Function*"},{"metadata":{"trusted":false},"cell_type":"code","source":"while True:\n    ret, frame = cam.read() \n    frame = imutils.resize(frame, width=800)\n    frame = cv2.flip(frame, 1)\n    clone = frame.copy()\n    roi = frame[top:bottom, right:left]\n    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) #converting the coloured image to BGR2GRAY\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n    number = \"Hello\"\n    if frame_no < 50: #for first 50 frames running average is executed\n        run_avg(gray, ra_weight)\n    else:\n        hand = segment(gray)\n        if hand is not None:\n            (threshold, segmented) = hand\n            cv2.drawContours(clone, [segmented + (right, top)], -1, (255, 255, 0)) #right and top are to move the contours from (0,0)\n            im = Image.fromarray(threshold)\n            im = im.resize((240,200),Image.ANTIALIAS)\n            im = np.array(im)\n            im = np.expand_dims(im,axis = 2)\n            im = np.expand_dims(im,axis = 0)\n            cv2.imshow(\"emojis\",emojis[model.predict_classes(im)[0]]) #model.predict returns a string with emoji name and extracts the emoji with that string to print it\n    cv2.rectangle(clone, (left, top), (right, bottom), (0,0,0), 2)\n    cv2.putText(clone,number,(0,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n    cv2.imshow(\"Video Feed\", clone)\n    k = cv2.waitKey(1)\n    \n    \ncam.release()\ncv2.destroyAllWindows()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}